---
title: "STA 380: Predictive Modeling, Part 2 Exercises"
output: md_document
---

# Group Member: Kelly Zhang, Rui Ying, He Wang

## Visual story telling part 1: green buildings
Step1: Filter out the builidng with leasing rate less than 10% 
```{r}
library(tidyverse)
library(ggplot2)
df_greenbuildings <- read.csv("greenbuildings.csv")
df_greenbuildings <- df_greenbuildings[(df_greenbuildings$leasing_rate > 10),]
```

Step2: Plot size versus rent to see if the rent affect by size. The scatter plot shows that as the the size increase, the rent increases as well. 
```{r}
ggplot(data = df_greenbuildings) + geom_point(aes(x=size, y=Rent, color = factor(green_rating))) +theme_bw()
```

Step3: Filter the data by size. Since the building we plan to build is 250,000 square feet, buildings with size between 200,000 to 300,000 square feet are good approximations for our analysis. A boxplot is then created to show the distribution of rent for buildings with size between 200,000 to 300,000 square feet and surprisingly the non-green actually slightly outperforms green buildings in rent price for size from 200000 to 250000.
```{r}
df_greenbuildings <-df_greenbuildings[df_greenbuildings$size > 200000 & df_greenbuildings$size < 300000,]
```
```{r}
df <- df_greenbuildings %>% mutate(sizecut = cut(size, c(200000,250000, 300000)))
ggplot(data= df) + geom_boxplot(aes(x = sizecut, y = Rent,fill=factor(df$green_rating))) +theme_bw()
```

Step4: To see how the age of building affects the rent, age(continous) variable is converted into 6 categories for plotting boxplots. The result shows that green-certified buildings have lower average rent than those non-green buildings, given the condition that we as an investor only cares about the return for 30 years. The average rent for green buildings exceeds non-green buildings only after 30 years. 
```{r}
df_greenbuildings <- df_greenbuildings %>% mutate(agecut = cut(age, c(-1,15,30,60,90,150, 250)))
ggplot(data= df_greenbuildings) + geom_boxplot(aes(x = agecut, y = df$Rent,fill=factor(green_rating))) +theme_bw()
```

Step5: By plotting the stories verusu rent, we see a clear trend of increase in rent as the number of stories increase
```{r}
ggplot(data = df_greenbuildings) + geom_point(aes(x=stories, y=Rent, color = factor(green_rating)))
```

Step6: Since we only care about the rent for the buildings that are similar to the one we are investing, the average rent for the buildings with 10-20 stories is higher for non-green versus green
```{r}
df1<- df_greenbuildings%>%mutate(storiescut = cut(stories, c(0,10,20,30,40)))
ggplot(data = df1) + geom_boxplot(aes(x= storiescut, y=Rent, fill = factor(green_rating)))+ theme_bw()
```

Step7: Combine the effect of age and stories variable by filter out the buildings with age greater than 30 and stories less than 10 and greater than 20. 
```{r}
df_greenbuildings <- df_greenbuildings[df_greenbuildings$age >-1 & df_greenbuildings$age<30,]
df_greenbuildings <- df_greenbuildings[df_greenbuildings$stories >10 & df_greenbuildings$stories<20,]
```

Step8: The fianl average of leasing rate for green buildings is 87.12074%, while the leasing rate for non-green buildings is 90.22357%. However, the rent is much higher for non-green buildings after counting the effect of size, age and stories, which is 35.39262 and for green building is 30.77000. 

Revenue = Building size(250,000) * Leasing Rate * Rent Per Square Feet
Expense for Non-green is 100 million, and for green is 105 million
Thus, the gross profit for non-green buildings is much higher than green buildings. Has that been said, we do not support to invest in green building. 
```{r}
df_profit <- df_greenbuildings%>%group_by(green_rating)%>%summarise(avg_leasing_rate = mean(leasing_rate), avg_rent = mean(Rent))
df_profit %>% mutate(Revenue = 250000 * avg_leasing_rate / 100 * avg_rent, Expense = c(1000000, 1050000))%>% mutate(gross_profit = Revenue - Expense)
```

## Visual story telling part 2: flights at ABIA

``````{r}
library(ggplot2)
```
```{r}
#read the file and changed few variable into categorical variable
airline = read.csv('ABIA.csv',header = TRUE)
airline$Month = as.factor(airline$Month)
airline$DayofMonth = as.factor(airline$DayofMonth)
airline$DayOfWeek = as.factor(airline$DayOfWeek)
#Filter the data of flights that departure from Austin 
From = airline[airline$Origin == 'AUS',]
```

Overall we can see the volume of the flights from Austin to other city went down towards the end of the years.
```{r}
# Trun delay into categorical variable 
From$delay = ifelse(From$ArrDelay >=1,0,1)
From$delay = as.factor(From$delay)

#groupby month and count the number of flights per month
From_count = From %>% 
  group_by(Month) %>%  
  summarize(flight_count=n())
# plot the graph
ggplot(From_count)+
  geom_bar(stat = 'identity',aes(x = Month, y = flight_count))+
  ggtitle("Flight volume per month")+theme(plot.title = element_text(hjust = 0.5))
```

Looking into it further we can see that even though June has the most flight volume, June has the lowest delay count. On the other hand September to November has the highest delay rate despite the fact that the flight volume are on the lower side.
```{r}
#grouped data by month and delays and count the delays in each month
From_dely = From %>%
  group_by(Month,delay)%>%
  summarise(delay_count=n())
#merged the delay count per month with the total flight counts
From_merge = merge(From_dely,From_count,by='Month')
From_merge = na.omit(From_merge)
#plot the percentafe of delat vs. ontime
ggplot(From_merge)+
  geom_bar(stat = 'identity',aes(x = Month, y = delay_count/flight_count, fill = factor(delay)))+
  ggtitle("Trend in delays per month(%)")+theme(plot.title = element_text(hjust = 0.5))
```

Now lets look in to what time was the worst to fly out. We can see Morning from 6-9am is when most of the delay happen followed by afternoon 12-3pm and before noon 9-12am and evening 3-6pm.
```{r}
#Grouped time into 7 time frame
From$time_frame = NA
From$time_frame[From$CRSDepTime > 600] = "EarlyMorning"
From$time_frame[From$CRSDepTime > 600 & From$CRSDepTime <= 900] = "Morning"
From$time_frame[From$CRSDepTime > 900 & From$CRSDepTime <= 1200] = "BeforeNoon"
From$time_frame[From$CRSDepTime > 1200 & From$CRSDepTime <= 1500] = "Afternoon"
From$time_frame[From$CRSDepTime > 1500 & From$CRSDepTime <= 1800] = "Evening"
From$time_frame[From$CRSDepTime > 1800 & From$CRSDepTime <= 2100] = "Night"
From$time_frame[From$CRSDepTime > 2100 & From$CRSDepTime <= 2359] = "LateNight"
#group by delays and time frame and get the count in each variable
sum_time = From %>%
  group_by(delay,time_frame)%>%
  summarise( time_count= n())
sum_time= na.omit(sum_time)
#plot the graph
ggplot(sum_time)+
  geom_bar(stat = 'identity',aes(x = time_frame, y = time_count,fill = factor(delay)),position = 'dodge')+
  ggtitle("Trend in delays on time of the day")+theme(plot.title = element_text(hjust = 0.5))
```

Now lets look at the average time of day in each time of the day. We see that the longest delays happens in the evening and the shortest happens in late night. If you leave on evning there are about 50% of chance that the flight delays for about 60 minutes.
```{r}
# grouped by time frame and calculated average time 
sum_time1 = na.omit(From) %>%
  group_by(time_frame)%>%
  summarise( time_mean = mean(ArrDelay),n= n())
ggplot(sum_time1)+
  geom_bar(stat = 'identity',aes(x = time_frame, y = time_mean))+
  ggtitle("Average time of Delay (Time of the Day)")+theme(plot.title = element_text(hjust = 0.5))
```

Now lets see which day has the worst. Overall either one of the day there are more probablity of delay than on time. However the worst day to go is saturday. 
```{r}
# changed dat of week in to words
From$name = NA
From$name[From$DayOfWeek == 1] = 'Monday'
From$name[From$DayOfWeek == 2] = 'Tuesday'
From$name[From$DayOfWeek == 3] = 'Wednesday'
From$name[From$DayOfWeek == 4] = 'Thursday'
From$name[From$DayOfWeek == 5] = 'Firday'
From$name[From$DayOfWeek == 6] = 'Saturday'
From$name[From$DayOfWeek == 7] = 'Sunday'

#grouped by name and delay and count the delay for each day
sum_day = From %>%
  group_by(name,delay)%>%
  summarise(Day_count = n())
sum_day = na.omit(sum_day)
#plot the graph
ggplot(sum_day)+
  geom_bar(stat = 'identity',aes(x = name, y = Day_count,fill = factor(delay)),position = 'dodge')+
  ggtitle("Delays in Day of the Week")+theme(plot.title = element_text(hjust = 0.5))
```

The average time of delays seems pretty even among different days of the week
```{r}
#grouped by names and calculated average time 
sum_day1 = na.omit(From) %>%
  group_by(name)%>%
  summarise(Day_mean=mean(ArrDelay),n = n())

ggplot(sum_day1)+
  geom_bar(stat = 'identity',aes(x = name, y = Day_mean))+
  ggtitle("Average time of delays in Day of the Week")+theme(plot.title = element_text(hjust = 0.5))

```

From the plot we can see WN has most the flights but also have a pretty high delay rate. OH is the only carrier that has a higher on time rate than delay rate, but they also have a very small number of flights. Time wise US has the lowest average delay time, OH and F9 has some what lower average. Other carrier seems to be somewhat similar in terms of delay time. 
```{r}
sum_car = From %>%
  group_by(UniqueCarrier,delay)%>%
  summarise(car_count = n())
sum_car = na.omit(sum_car)

ggplot(sum_car)+
  geom_bar(stat = 'identity',aes(x = UniqueCarrier, y = car_count,fill = factor(delay)),position = 'dodge' )+
  ggtitle("Delay in each Carrier")+theme(plot.title = element_text(hjust = 0.5))

sum_car1 = na.omit(From) %>%
  group_by(UniqueCarrier)%>%
  summarise(car_mean=mean(ArrDelay),n = n())

ggplot(sum_car1)+
  geom_bar(stat = 'identity',aes(x = UniqueCarrier , y = car_mean))+
  ggtitle("Average Delay in each Carrier")+theme(plot.title = element_text(hjust = 0.5))
```
```{r}
df1<- From[(From$CarrierDelay == 1),] 
df1 = na.omit(df1)
df1_1 = df1%>%mutate(delay_reason = "CarrierDelay")

df2<-From[(From$WeatherDelay == 1),] 
df2 = na.omit(df2)
df2_2 <- df2%>%mutate(delay_reason = "WeatherDelay")

df3<-From[(From$NASDelay == 1),] 
df3 = na.omit(df3)
df3_3 = df3%>%mutate(delay_reason = "NASDelay")

df4<-From[(From$SecurityDelay == 1),] 
df4 = na.omit(df4)
df4_4 = df4%>%mutate(delay_reason = "SecurityDelay")

df5<-From[(From$SecurityDelay == 1),] 
df5 = na.omit(df5)
df5_5 = df5%>%mutate(delay_reason = "LateAircraftDelay")

df <- rbind(df1_1,df2_2,df3_3,df4_4,df5_5)
```

From the heatmap we can see, after taking out the early departure time, that evening is worst time to fly in terms of average time on delay.
```{r}
h1 = From[,c("time_frame","DayOfWeek","ArrDelay")]
h1 = h1[h1['ArrDelay']>=0,]
h1 = h1%>% group_by(DayOfWeek,time_frame)%>%summarise(delay_mean = mean(ArrDelay),n=n())
h1 = na.omit(h1)
ggplot(h1, aes(DayOfWeek, time_frame)) + geom_tile(aes(fill = delay_mean), colour = "white") + scale_fill_gradient(low = "white",high = "steelblue")
```

If you can avoid NW on Wednesday, it also shows that a lot of carriers tends to have longer delays over the weekends.
```{r}
h2 = From[,c("UniqueCarrier","DayOfWeek","ArrDelay")]
h2 = h2[h2['ArrDelay']>=0,]
h2 = h2%>% group_by(DayOfWeek,UniqueCarrier)%>%summarise(delay_mean = mean(ArrDelay),n=n())
h2 = na.omit(h2)
ggplot(h2, aes(DayOfWeek, UniqueCarrier)) + geom_tile(aes(fill = delay_mean), colour = "white") + scale_fill_gradient(low = "white",high = "steelblue")
```

As expected December have longer delays especailly on saturdays. Surprisingly, despite the fact that September to Novenmber has the most delays, the average delay time was the shortest.
```{r}
h3 = From[,c("Month","DayOfWeek","ArrDelay")]
h3 = h3[h3['ArrDelay']>=0,]
h3 = h3%>% group_by(DayOfWeek,Month)%>%summarise(delay_mean = mean(ArrDelay),n=n())
h3 = na.omit(h3)
ggplot(h3, aes(Month,DayOfWeek)) + geom_tile(aes(fill = delay_mean), colour = "white") + scale_fill_gradient(low = "white",high = "steelblue")
```

From the calender we can see December is where most of longer delay happens, maybe due to more people travel on the holidays. We see a particularly long delay on march the 18th so we looked up the date and see anything happened that day, it turns out that to be the last day of the SXSW. Understandably, since people come in different days but might all leave on the same day cuasing a lot of traffic. 
```{r}
h4 = From[,c("Month","DayofMonth","ArrDelay")]
h4 = h4[h4['ArrDelay']>0,]
h4 = h4 %>% group_by(Month,DayofMonth)%>% summarise(day1_mean = mean(ArrDelay),n=n())
h4 = na.omit(h4)
ggplot(h4, aes(Month,DayofMonth)) + geom_tile(aes(fill = day1_mean), colour = "white") + scale_fill_gradient(low = "white",high = "steelblue")
```

Lastly we can plotted the reason of delays in each month, since a lot of data were missing, most of the cause was carrier delays and NASDelays.
```{r}
h5 = df[,c("Month","delay_reason")]
h5 = h5 %>% group_by(Month,delay_reason)%>% summarise(n=n())
h5 = na.omit(h5)
ggplot(h5) + geom_bar(stat = 'identity', mapping = aes(x=Month,y=n,fill = delay_reason),position = 'dodge')
```

Conclusion: next time when you want to book for a flight, you can refer to these information to aviod delays or at least choose a time of shorter delay time.


## Portfolio modeling
Step1: In order to set up ETFs portfolios and analyze short-term tail risk, we choose three different portfolios with different industries. For the first portfolio which is pretty diverse, it is included ETFs from 3 industries: Agricultural commodity, Metals and Healthcare, and ETFs from 2 different supporters: government and corporate. 
```{r portfolio_1}
library(quantmod)
library(mosaic)
library(foreach)
library(tidyverse)
# Import ETF from different industries and different functions
# Agriculture, metal, healthcare, government, corporate
# pretty safe
set.seed(9)
portfolio_1 = c("LQD", "TAGS", "DBB","SHV","IHI")
getSymbols(portfolio_1, from = "2015-01-01") 

# Adjust for splits and dividends
LQDa = adjustOHLC(LQD)
TAGSa = adjustOHLC(TAGS)
DBBa = adjustOHLC(DBB)
SHVa = adjustOHLC(SHV)
IHIa = adjustOHLC(IHI)
# Look at close-to-close changes
plot(ClCl(DBBa))
title('Close-to-Close Changes for DBB')
set.seed(9)
# Combine close to close changes in a single matrix
all_returns_1 = cbind(ClCl(LQDa),ClCl(TAGSa),ClCl(DBBa),ClCl(SHVa),ClCl(IHIa))
head(all_returns_1)
all_returns_1 = as.matrix(na.omit(all_returns_1))
N = nrow(all_returns_1)

pairs(all_returns_1)
title('Correlationship between ETFs in portfolio 1',line=5)

# Look at the portfolio_1 returns over time
plot(all_returns_1[,5], type='l')

# are today's returns correlated with tomorrow's? 
# See today's return and tomorrow's for one ETFs  
plot(all_returns_1[1:(N-1),5], all_returns_1[2:N,5])
title("Today's return vs Tomorrow's return for IHI",line=5)

for(ticker in portfolio_1) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

head(LQDa)
# Sample a random return from the empirical joint distribution
# This simulates a random day
set.seed(9)
return.today = resample(all_returns_1, 1, orig.ids=FALSE)
initial_wealth = 100000
sim1 = foreach(i=1:50, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_1, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

head(sim1)
hist(sim1[,n_days], 25)
title("Capital Changes for portfolio 2",line=5)
# Profit/loss
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30)
title("Returns Or Loss",line=5)
mean(sim1[,n_days] > 100000)
quantile(sim1[,n_days]- initial_wealth,.05)
quantile(sim1[,n_days]- initial_wealth,.01)
```

The mean of capital after 20 trading days is 100708.6 and we can earn an average rate of return of 0.7% for 20 trading days on our investment. In addition, we could earn returns at the 58% confidence. Considering the VaR, if the degree of risk preference and acceptance ability of our investors is 5%, portfolio 1 has a 5% VaR of 3128, which means that there is a 0.05 probability that the portfolio 1 will fall in value by more than 3128 in a 20 trading-day period. If the degree of risk preference and acceptance ability of our investors is 1%, portfolio 1 has a 1% VaR of 4396, which means that there is a 0.01 probability that the portfolio 1 will fall in value by more than 4396 in a 20 trading-day period. 

Step2: Portfolio 2 is more aggressive. It contains two kinds of bonds, which are volitality and leverage bonds. Leverage bonds provide magnified exposure to popular fixed income benchmarks. They can generage amplified returns and also have higher risks. Volitality ETFs tend to move in the opposite direction of the broad market. Therefore, the portfolio 2 is supposed to win the market and win more when the market is down. However, it has to cover more risks as well whe nthe market is up.

```{r portfolio_2}
portfolio_2 = c("VIXY", "VIXM", "VIIX","TBT","TMV")
getSymbols(portfolio_2, from = "2015-01-01") 
set.seed(9)
# Adjust for splits and dividends
VIXYa = adjustOHLC(VIXY)
VIXMa = adjustOHLC(VIXM)
VIIXa = adjustOHLC(TBT)
TMVa = adjustOHLC(TMV)
TBTa = adjustOHLC(TBT)
# Look at close-to-close changes
plot(ClCl(VIXYa))
title('Close-to-Close Changes for VIXY',line=5)
set.seed(9)
all_returns_2 = cbind(ClCl(VIXYa),ClCl(VIXMa),ClCl(VIIXa),ClCl(TMVa),ClCl(TBTa))
head(all_returns_2)
all_returns_2 = as.matrix(na.omit(all_returns_2))
N = nrow(all_returns_2)

pairs(all_returns_2)
title('Correlationship between ETFs in portfolio 2',line=5)

# Look at the portfolio_2 returns over time
plot(all_returns_2[,5], type='l')

# are today's returns correlated with tomorrow's? 
plot(all_returns_2[1:(N-1),5], all_returns_2[2:N,5])
title("Today's return vs Tomorrow's return for TBT",line=5)

for(ticker in portfolio_2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

head(TBTa)
set.seed(9)
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns_2, 1, orig.ids=FALSE)

initial_wealth = 100000
sim2 = foreach(i=1:50, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_2, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

head(sim2)
hist(sim2[,n_days], 25)
title("Capital Changes for portfolio 2",line=5)
# Profit/loss
mean(sim2[,n_days])
hist(sim2[,n_days]-initial_wealth, breaks=30)
title("Returns Or Loss",line=5)
mean(sim2[,n_days] > 100000)
quantile(sim2[,n_days]-initial_wealth,.05)
quantile(sim2[,n_days]-initial_wealth,.01)
```

If we invest portfolio 2, the mean of capital is only 96862 for 20 trading days, which has the 3.1% loss. And there is only 34% probability to win. For investors at 5% of risk preference, the value in risk is more than 15539. For investors at 1% of risk preference, the value in risk is more than 17789. It only earns when the market is down, so we need to pay close attention to the broad market when investing this portfolio. 


Step3: Finally, we choose a safer portfolio containing 5 government bonds. Government Bonds ETFs offer investors exposure to fixed income securities issued by government agencies, which have little risk and small returns as well. They are more preferred by risk averse individuals.
```{r portfolio_3}
portfolio_3 = c("IEF", "SHY", "BIL","GOVT","SCHO")
getSymbols(portfolio_3, from = "2015-01-01") 
# Adjust for splits and dividends
IEFa = adjustOHLC(IEF)
SHYa = adjustOHLC(SHY)
BILa = adjustOHLC(BIL)
GOVTa = adjustOHLC(GOVT)
SCHOa = adjustOHLC(SCHO)
# Look at close-to-close changes
plot(ClCl(SCHOa))
title('Close-to-Close Changes for SCHO',line=5)
set.seed(99)

all_returns_3 = cbind(ClCl(IEFa),ClCl(SHYa),ClCl(BILa),ClCl(GOVTa),ClCl(SCHOa))
head(all_returns_3)
all_returns_3 = as.matrix(na.omit(all_returns_3))
N = nrow(all_returns_3)

pairs(all_returns_3)
title('Correlationship between ETFs in portfolio 3',line=5)
# all related. Because it is in the same industry
plot(all_returns_3[,1], type='l')

# Look at the portfolio_3 returns over time
plot(all_returns_3[,5], type='l')

# are today's returns correlated with tomorrow's? 
plot(all_returns_1[1:(N-1),5], all_returns_1[2:N,5])
title("Today's return vs Tomorrow's return for SCHO",line=5)

for(ticker in portfolio_3) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}


# Sample a random return from the empirical joint distribution
# This simulates a random day
set.seed(99)
return.today = resample(all_returns_3, 1, orig.ids=FALSE)

initial_wealth = 100000
sim3 = foreach(i=1:50, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns_3, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}

head(sim3)
hist(sim3[,n_days], 25)
title("Capital Changes for portfolio 3",line=5)
# Profit/loss
mean(sim3[,n_days])
hist(sim3[,n_days]- initial_wealth, breaks=30)
title("Returns Or Loss",line=5)
mean(sim3[,n_days] > 100000)
quantile(sim3[,n_days]- initial_wealth,.05)
quantile(sim3[,n_days]- initial_wealth,.01)
```

From the return and loss histogram, we know that the portfolio 3 is safer than portfolio 1 and 2, the most loss can only be 1500 and also the most earns can only less than 1200 approximately which follows the principle of low risk and low return. Also the correlationship between ETFs in porfolio 3, they have close relationship between each other since they are all issued by government agencies and varied simultaneously. For investors at 1% of risk preference, the value in risk is more than 1302.

Step4: Combine VaR for three portfolios
```{r}
# Combine VaR for three portfolios
Portfolio_3 <- c(quantile(sim3[,n_days]- initial_wealth,.05), quantile(sim3[,n_days]- initial_wealth,.01))
Portfolio_2 <- c(quantile(sim2[,n_days]- initial_wealth,.05), quantile(sim2[,n_days]- initial_wealth,.01))
Portfolio_1<- c(quantile(sim1[,n_days]- initial_wealth,.05), quantile(sim1[,n_days]- initial_wealth,.01))
VaR <- rbind(Portfolio_1,Portfolio_2,Portfolio_3)
dimnames(VaR) = list( c("Portfolio 1","Portfolio 2","Portfolio 3"),c("5%","1%"))
VaR

barplot(VaR, beside = TRUE,legend=TRUE,col=c("darkblue","grey","orange"),cex.names=0.8,las=1,bty ="n",args.legend = list(x ='bottom', bty='n', inset=c(-0.25,0)))
title("VaR For 3 Portfolio at 5% and 1%")

```

From the VaR at 5% and 1% for these 3 portfolios, we can see that the porfolio 2 loses most and covers the most risks and the porfolio 1 is the safest compared with other porfolios. Also, the value at risk is also different by the lose probability. 1% lose probability will lose more compared with 5% lose probability. And the probability also represents the risk preference of investors. If the investors have low risk preference, it means that we need to consider high lose probability and the VaR turns out to be less. And we need to choose safer portfolio, like portfolio 1 or 2, in order to hedge risk as much as possible.

## Market segmentation
Step 1: Find the correlationship between different interests
```{r readfile}
tweet = read.csv("social_marketing.csv", na.strings = '')
library(tidyverse) 
library(cluster)   
library(corrplot)
library(ggplot2)
library(factoextra)
library(NbClust)
library(gridExtra)
str(tweet)
# see the correlationship between different interests.
tweet_cor <- cor(tweet[c(2:37)])
corrplot(tweet_cor,method = 'shade',type = 'upper')
```

From the corrplot of different interests, we could see that the following combination has strong relationship
1)online_gaming and college_uni
2)health_nutrition and personal_fitness
3)beauty and fasion
4)cooking and fasion
5)religion and parenting

However, there are some interets thate are not related to any interest, like 'uncategorized', 'current event', etc. Therefore, we need to delete those interest in order to prepare a concise report for NutrientH20. We delete the following interests: 'chatter' , 'uncategorized'.

Step 2: Pre-processing data
```{r preprossing data}
tweet_new <- tweet[,c(3:5,7:37)]
tweet_scaled = scale(tweet_new, center=TRUE, scale=TRUE) 
mu = attr(tweet_scaled,"scaled:center")
sigma = attr(tweet_scaled,"scaled:scale")
```

Step 3: See how many clusters should we choose
```{r kmeans to see how many clusters should we choose}
set.seed(9)
clust2 = kmeans(tweet_scaled, 2, nstart=25)
clus2plot = fviz_cluster(clust2, data = tweet_scaled, 
                         ellipse.type = "euclid", # Concentration ellipse
                         ggtheme = theme_classic(),geom = c("point")
)
set.seed(9)
clust4 = kmeans(tweet_scaled, 4, nstart=25)
clus4plot = fviz_cluster(clust4, data = tweet_scaled, 
                         ellipse.type = "euclid", # Concentration ellipse
                         ggtheme = theme_classic(),geom = c("point")
)
set.seed(9)
clust6 = kmeans(tweet_scaled, 6, nstart=25)
clus6plot = fviz_cluster(clust6, data = tweet_scaled, 
                         ellipse.type = "euclid", # Concentration ellipse
                         ggtheme = theme_classic(),geom = c("point")
)
set.seed(9)
clust8 = kmeans(tweet_scaled, 8, nstart=25)
clus8plot = fviz_cluster(clust8, data = tweet_scaled, 
                         ellipse.type = "euclid", # Concentration ellipse
                         ggtheme = theme_classic(),geom = c("point")
)

grid.arrange(clus2plot,clus4plot,clus6plot,clus8plot,ncol = 2,nrow=2)
```

Compared to different clusters, we should choose 6 clusters which divide clearer clusters. Then we need to analyze each market and look into each interest for each cluster.

Step 4: Analyze each cluster
```{r Plot the 6 clusters interest}
set.seed(10)
clust6 = kmeans(tweet_scaled, 6, nstart=25)
#Choose top 8 interests for each cluster
cluster1 = sort(clust6$center[1,]*sigma + mu,decreasing = TRUE)[0:8] 
cluster2 = sort(clust6$center[2,]*sigma + mu,decreasing = TRUE)[0:8] 
cluster3 = sort(clust6$center[3,]*sigma + mu,decreasing = TRUE)[0:8] 
cluster4 = sort(clust6$center[4,]*sigma + mu,decreasing = TRUE)[0:8] 
cluster5 = sort(clust6$center[5,]*sigma + mu,decreasing = TRUE)[0:8] 
cluster6 = sort(clust6$center[6,]*sigma + mu,decreasing = TRUE)[0:8] 
par(mfrow=c(3,2))
barplot(cluster1, col = 'green',las=2, cex.names=1, main= 'Cluster 1')
barplot(cluster2, col = 'orange',las=2, cex.names=1., main= 'Cluster 2')
barplot(cluster3, col = 'red', las=2, cex.names=1, main= 'Cluster 3')
barplot(cluster4, col = 'grey', las=2, cex.names=1,main= 'Cluster 4')
barplot(cluster5, col = 'blue', las=2, cex.names=1, main= 'Cluster 5')
barplot(cluster6, col = 'black',las=2, cex.names=1, main= 'Cluster 6')

```

5 Distinct Markets
We cannot conclude a specific group for cluster 3, so there are 5 interested groups in total for NutrientH20 (skip cluster 3). From this 5 clusters, we can make the market segment clearly as following:
1)**Interested in healthy food and cook **[Cluster 1] : the market targets at people who are interested in cooking and focus on healthy food and personal fitness. They also prefer outdoor activities and photo sharing, maybe sharing their healthy food pictures.
2)**College Students who like entainment and sports **[Cluster 2] : the market targets at college students, that are mostly interested in online gaming. The interested group may mostly contain college students.
3)**Photo sharing people **[Cluster 4] : the market targets at people who are used to share their photos and are more concerned about current events. They may share photos of shopping, travelling and other interesting life.
4)**Concerned about Politics **[Cluster 5] : the market targets at people who pay close attention to politics, news, sports, computers and current events. They may contain more middle-aged males.
5)**Housewife **[Cluster 6] : the market targets at people who love cooking, fashion and beauty. This market is represented by housewives who are interested in cooking, beauty and shopping.


## Author attribution
```{r}
library(tm) 
library(naivebayes)
library(e1071)
library(tidyverse)
```

Step1: First we created reader plain function to help us read the file. Then read the train and test datasets and create corpus for each sets.
```{r}
readerPlain = function(fname){
    readPlain(elem=list(content=readLines(fname)), 
       id=fname, language='en') }

#read the training data 
file_train = Sys.glob('ReutersC50/C50train/*/*.txt')
doc_train = lapply(file_train, readerPlain)

#read the testing data 
file_test = Sys.glob('ReutersC50/C50test/*/*.txt')
doc_test = lapply(file_test, readerPlain)
```

Step2: Go through each of the data and extract the author names 
```{r}
mynames = file_train %>%
  { strsplit(., '/', fixed=TRUE) } 

train_authors= NULL
for (i in mynames){
  train_authors = c(train_authors, i[3])
}

mynames = file_test %>%
  { strsplit(., '/', fixed=TRUE) } 

test_authors= NULL
for (i in mynames){
  test_authors = c(test_authors, i[3])
}

```

Step3: Create corpus for both data sets, and also make everything to lowercase. Remove numbers, remove punctuations, remove all white spaces, and remove all the stop words.
```{r}
documents_raw_train = Corpus(VectorSource(doc_train))
documents_raw_test = Corpus(VectorSource(doc_test))

my_documents = documents_raw_train
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))

test_documents = documents_raw_test
test_documents = tm_map(test_documents, content_transformer(tolower)) # make everything lowercase
test_documents = tm_map(test_documents, content_transformer(removeNumbers)) # remove numbers
test_documents = tm_map(test_documents, content_transformer(removePunctuation)) # remove punctuation
test_documents = tm_map(test_documents, content_transformer(stripWhitespace)) ## remove excess white-space
test_documents = tm_map(test_documents, content_transformer(removeWords), stopwords("en"))
```

Step4: After breifly cleaned the data, we turn it into a matrix. We see the sparsity is very high, so we decided to remove all elements which have more then sparse factir of 0.975. We then created matrixs using the revised data. In addition, we computed the TF-IDF test on both the test and train and turn it into a matrix.
```{r}
DTM_train = DocumentTermMatrix(my_documents)
DTM_train = removeSparseTerms(DTM_train, 0.975)

DTM_test = DocumentTermMatrix(test_documents)
DTM_test = removeSparseTerms(DTM_test, 0.975)

tfidf_train = weightTfIdf(DTM_train)
tfidf_test = weightTfIdf(DTM_test)

nb_train = as.matrix(DTM_train)
nb_test = as.matrix(DTM_test)

x_train = as.matrix(tfidf_train)
x_test = as.matrix(tfidf_test)
```

Step5: Our data is almost ready at this point, we then combined our author names and the word counts into one matrix and turned it into dataframe. We did the same thing again for author names and word count. One more step we did is to rename the author column for easier access.
```{r}
train1 = cbind(x_train,train_authors)
test1 = cbind(x_test,test_authors) 

nb_train = cbind(nb_train,train_authors)
nb_test = cbind(nb_test,test_authors)

train = as.data.frame(train1)
names(train)[1601]<-"author_target"
test = as.data.frame(test1)
names(test)[1632]<-'author_target'

nbtrain = as.data.frame(nb_train)
names(nbtrain)[1601]<-"author_target"
nbtest = as.data.frame(nb_test)
names(nbtest)[1632]<-'author_target'
```

Step6: Our final step is to check for any missing words in the test sets using intersect function. 
```{r}
intersection = intersect(names(train),names(test))
train = train[,intersection]
test = test[,intersection]

intersection = intersect(names(nbtrain),names(nbtest))
nbtrain = nbtrain[,intersection]
nbtest = nbtest[,intersection]
```

Step7: After getting our code ready, we decied to perform a random forest to predict for the correct author. We tried 100,500,1000 trees, and 50, 80, 100 for m. Using 500 tree and m = 80 give us the best result without too much compelxity. So our best model accuracy is about 80.48%.
```{r}
library(randomForest)
set.seed(1)
rf.fit = randomForest(y = factor(train_authors), x = x_train,ntree=500,mtry = 80)
rf.pred = predict(rf.fit, data=test)
mean(rf.pred == test$author_target)
```

Step8: We also tried the naive bayes method to test for accuracy, but we only got about 69% accuracy. 
```{r}
nb.fit = naiveBayes(y = factor(train_authors), x = nbtrain)
nb.pred = predict(nb.fit, newdata = nbtest)
mean(nb.pred == test$author_target)
```

Overall, Random Forest turns out to be the best model that we did; it give about 80% accuracy in predicting the right author.


## Association Rule Mining
Step1: Read the transaction and look at rules with support > 0.005, confidence >0.1 & length of the number of items smaller than or equal to 5. It returns with 1582 rules. The graph shows confidence versus support and is colored by lift. The rules with high lift are those with low support. The rules with one or two products have relatively lower confidence, whereas rules with three or four products have relatively lower support. However,  1582 rules are to much for us to analyze or get useful insights.
```{r}
library(tidyverse)
library(arules)  
library(arulesViz)
groceries= read.transactions("groceries.txt", rm.duplicates=TRUE, format="basket", sep=',')
groceriesrules = apriori(groceries, 
	parameter=list(support=0.005, confidence=0.1, maxlen=4))
plot(groceriesrules)
plot(groceriesrules, method='two-key plot')
```

Step2: Reset lift, confidence and support.
1) Set lift greater than 2.2 since the highest lift is around 3. Lift is important to determine if a certain product is more or less likely to be purchased with other products than a random draw. 
2) Confidence greater than 0.4. Too small confidence gives us too many rules still.
3) Support greater than 0.01, meaning at least one out of 100 people buy this porduct. Otherwise the products are not prevalent enough and leave us too little data for analysis. 

Based on these three measurements, we got 16 rules.
Intepretation of Rule 1 as an example
Support = 0.014: 1.4% of transactions contain "other vegetables".
Confidence: 45% of the transactions that contain onion also contain "other vegetables". 
Lift: 2.37: Customer who bought "onion" is at least twice more likely to buy "other vegetables" than a random customer do.
```{r}
inspect(subset(groceriesrules, subset=lift > 2.2 & confidence > 0.4 & support > 0.01))
```

Step3: Plot the subset of rules by defined measurement in step2. Customers usually buy citrus fruit, yogurt, tropical fruit, rolls/buns or domestic eggs together with Whole milk or other vegetables, which actually make sense because those food are all for daily use. Thus, the grocery store could consider placing those items together in physical stores. In addition, the company could provide customized recommendations/advertisements for online shoppers.
```{r}
# graph-based visualization
sub1 = subset(groceriesrules, subset=lift > 2 & confidence > 0.4 & support > 0.01)
plot(sub1, method='graph')
```

